{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2295f5",
   "metadata": {},
   "source": [
    "## IMPORT & KONFIGURASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee6c2518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English AI Models...\n",
      "Models loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "FOLDER_VIDEO = 'Vid'  # Pastikan folder ini berisi video interview bahasa Inggris\n",
    "MODEL_WHISPER_SIZE = 'base' \n",
    "\n",
    "# GANTI 1: Menggunakan model Sentence-BERT khusus Bahasa Inggris\n",
    "# Model ini lebih ringan dan performanya sangat baik untuk teks Inggris\n",
    "MODEL_SBERT = 'all-MiniLM-L6-v2' \n",
    "\n",
    "# --- DATASET KUNCI JAWABAN (ENGLISH VERSION) ---\n",
    "kunci_jawaban_db = {\n",
    "    'interview_question_1': {\n",
    "        'ideal_answer': \"Machine Learning is a subset of artificial intelligence that focuses on building systems that learn from data to improve their performance without being explicitly programmed.\",\n",
    "        'keywords': ['learn from data', 'improve', 'explicitly programmed']\n",
    "    },\n",
    "    'interview_question_2': {\n",
    "        'ideal_answer': \"In supervised learning, the algorithm is trained on labeled data, while unsupervised learning deals with unlabeled data to find hidden patterns.\",\n",
    "        'keywords': ['labeled data', 'unlabeled', 'patterns']\n",
    "    },\n",
    "    'interview_question_3': {\n",
    "        'ideal_answer': \"In supervised learning, the algorithm is trained on labeled data, while unsupervised learning deals with unlabeled data to find hidden patterns.\",\n",
    "        'keywords': ['labeled data', 'unlabeled', 'patterns']\n",
    "    },\n",
    "    # Sesuaikan nama key ini dengan nama file video kamu\n",
    "}\n",
    "\n",
    "# --- FUNGSI LOAD MODEL ---\n",
    "print(\"Loading English AI Models...\")\n",
    "stt_model = whisper.load_model(MODEL_WHISPER_SIZE)\n",
    "nlp_model = SentenceTransformer(MODEL_SBERT)\n",
    "print(\"Models loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a294f",
   "metadata": {},
   "source": [
    "## HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d68d80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_text_detail(transkrip, keywords_wajib):\n",
    "    \"\"\"\n",
    "    Menganalisis Filler Words, Hedging Words (Keraguan), dan Keywords.\n",
    "    \"\"\"\n",
    "    transkrip_lower = transkrip.lower()\n",
    "    \n",
    "    # 1. Cek Filler Words (Indikator gugup)\n",
    "    filler_list = ['um', 'uh', 'like', 'you know', 'actually', 'basically', 'i mean', 'ah', 'er']\n",
    "    filler_count = 0\n",
    "    \n",
    "    for filler in filler_list:\n",
    "        # Tambah spasi agar tidak mendeteksi kata di dalam kata lain\n",
    "        count = transkrip_lower.count(f\" {filler} \") \n",
    "        if count > 0:\n",
    "            filler_count += count\n",
    "            \n",
    "    # 2. Cek Hedging Words (Indikator ketidakyakinan)\n",
    "    # Kata-kata ini menunjukkan kandidat tidak yakin dengan jawabannya\n",
    "    hedging_list = ['maybe', 'i think', 'probably', 'sort of', 'kind of', 'i guess', 'perhaps']\n",
    "    hedging_count = 0\n",
    "    found_hedging = []\n",
    "    \n",
    "    for word in hedging_list:\n",
    "        if f\" {word} \" in transkrip_lower:\n",
    "            count = transkrip_lower.count(f\" {word} \")\n",
    "            hedging_count += count\n",
    "            found_hedging.append(word)\n",
    "\n",
    "    # 3. Cek Keywords\n",
    "    missed_keywords = []\n",
    "    hit_keywords = []\n",
    "    \n",
    "    for key in keywords_wajib:\n",
    "        if key.lower() in transkrip_lower:\n",
    "            hit_keywords.append(key)\n",
    "        else:\n",
    "            missed_keywords.append(key)\n",
    "            \n",
    "    # Hitung persentase keyword\n",
    "    if len(keywords_wajib) == 0:\n",
    "        keyword_score = 100\n",
    "    else:\n",
    "        keyword_score = (len(hit_keywords) / len(keywords_wajib)) * 100\n",
    "        \n",
    "    return {\n",
    "        'filler_count': filler_count,\n",
    "        'hedging_count': hedging_count,\n",
    "        'hedging_words': \", \".join(found_hedging),\n",
    "        'missed_keywords': missed_keywords,\n",
    "        'keyword_score': keyword_score\n",
    "    }\n",
    "\n",
    "def hitung_confidence_score(wpm, filler_count, hedging_count):\n",
    "    \"\"\"\n",
    "    Menghitung skor kepercayaan diri (0-100) berdasarkan heuristik.\n",
    "    \"\"\"\n",
    "    base_score = 100\n",
    "    \n",
    "    # Penalti 1: Filler Words (Dikurangi 2 poin per filler)\n",
    "    base_score -= (filler_count * 2)\n",
    "    \n",
    "    # Penalti 2: Hedging Words (Dikurangi 3 poin per kata ragu)\n",
    "    base_score -= (hedging_count * 3)\n",
    "    \n",
    "    # Penalti 3: Kecepatan Bicara (WPM)\n",
    "    # Range ideal: 110 - 160 WPM\n",
    "    if wpm < 100: \n",
    "        # Terlalu lambat = penalti (ragu-ragu)\n",
    "        base_score -= (100 - wpm) * 0.5 \n",
    "    elif wpm > 170:\n",
    "        # Terlalu cepat = penalti (gugup/rushed)\n",
    "        base_score -= (wpm - 170) * 0.5\n",
    "        \n",
    "    # Pastikan skor tidak di bawah 0 atau di atas 100\n",
    "    return max(0, min(base_score, 100))\n",
    "\n",
    "def generate_automated_feedback(wpm, semantic_score, missed_keywords, confidence_score):\n",
    "    \"\"\"\n",
    "    Membuat feedback otomatis yang lebih cerdas.\n",
    "    \"\"\"\n",
    "    feedback = []\n",
    "    \n",
    "    # Feedback Confidence\n",
    "    if confidence_score >= 85:\n",
    "        feedback.append(\"⭐ Terdengar sangat percaya diri dan profesional.\")\n",
    "    elif confidence_score >= 70:\n",
    "        feedback.append(\"✅ Penyampaian cukup baik, namun kurangi kata pengisi (filler).\")\n",
    "    else:\n",
    "        feedback.append(\"⚠️ Terdengar ragu-ragu. Hindari kata 'umm/uhh' dan 'i think' agar lebih meyakinkan.\")\n",
    "        \n",
    "    # Feedback Kecepatan\n",
    "    if wpm < 110:\n",
    "        feedback.append(\"(Bicara sedikit lebih cepat agar lebih energik).\")\n",
    "    \n",
    "    # Feedback Konten\n",
    "    if len(missed_keywords) > 0:\n",
    "        feedback.append(f\"❌ Poin hilang: {', '.join(missed_keywords[:3])}...\")\n",
    "    \n",
    "    return \" \".join(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9b5af",
   "metadata": {},
   "source": [
    "## MAIN PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bda02abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proses_penilaian_interview_lengkap():\n",
    "    video_paths = glob.glob(os.path.join(FOLDER_VIDEO, '*.webm'))\n",
    "    video_paths.sort()\n",
    "    \n",
    "    laporan_detail = []\n",
    "\n",
    "    print(f\"Starting analysis with CONFIDENCE SCORE for {len(video_paths)} videos...\\\\n\")\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        filename = os.path.basename(video_path).replace('.webm', '')\n",
    "        \n",
    "        if filename not in kunci_jawaban_db:\n",
    "            print(f\"[SKIP] No answer key for: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"--> Processing: {filename}...\")\n",
    "        \n",
    "        # 1. Transcribe\n",
    "        result = stt_model.transcribe(video_path, language='en') \n",
    "        transkrip = result['text'].strip()\n",
    "        \n",
    "        # 2. Hitung WPM\n",
    "        if result['segments']:\n",
    "            durasi_detik = result['segments'][-1]['end']\n",
    "        else:\n",
    "            durasi_detik = 1.0\n",
    "            \n",
    "        jumlah_kata = len(transkrip.split())\n",
    "        wpm = (jumlah_kata / durasi_detik) * 60\n",
    "        \n",
    "        # 3. Load Kunci\n",
    "        data_kunci = kunci_jawaban_db[filename]\n",
    "        \n",
    "        # 4. Semantic Score\n",
    "        emb_kandidat = nlp_model.encode(transkrip, convert_to_tensor=True)\n",
    "        emb_ideal = nlp_model.encode(data_kunci['ideal_answer'], convert_to_tensor=True)\n",
    "        semantic_score = util.pytorch_cos_sim(emb_kandidat, emb_ideal).item() * 100\n",
    "        \n",
    "        # 5. Analisis Detail (Keywords, Fillers, Hedging)\n",
    "        analisis = analisis_text_detail(transkrip, data_kunci['keywords'])\n",
    "        \n",
    "        # 6. --- BARU: Hitung Confidence Score ---\n",
    "        confidence_score = hitung_confidence_score(\n",
    "            wpm, \n",
    "            analisis['filler_count'], \n",
    "            analisis['hedging_count']\n",
    "        )\n",
    "        \n",
    "        # 7. Final Score (Bobot disesuaikan: Semantic 50%, Keyword 30%, Confidence 20%)\n",
    "        final_score = (semantic_score * 0.5) + (analisis['keyword_score'] * 0.3) + (confidence_score * 0.2)\n",
    "        \n",
    "        # 8. Feedback\n",
    "        saran = generate_automated_feedback(\n",
    "            wpm, semantic_score, analisis['missed_keywords'], confidence_score\n",
    "        )\n",
    "\n",
    "        laporan_detail.append({\n",
    "            'Video ID': filename,\n",
    "            'Transcript': transkrip,\n",
    "            'WPM': int(wpm),\n",
    "            'Fillers': analisis['filler_count'],\n",
    "            'Hedging Words': analisis['hedging_count'], # Jumlah kata ragu\n",
    "            'Confidence Score': round(confidence_score, 1), # Skor baru\n",
    "            'Semantic Score': round(semantic_score, 1),\n",
    "            'Keyword Score': round(analisis['keyword_score'], 1),\n",
    "            'FINAL SCORE': round(final_score, 1),\n",
    "            'Feedback': saran\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(laporan_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2644a",
   "metadata": {},
   "source": [
    "## OUTPUT REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d24290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis with CONFIDENCE SCORE for 5 videos...\\n\n",
      "--> Processing: interview_question_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/miniconda3/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Processing: interview_question_2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/miniconda3/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Processing: interview_question_3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/miniconda3/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] No answer key for: interview_question_4\n",
      "[SKIP] No answer key for: interview_question_5\n",
      "\n",
      "=== AI INTERVIEW REPORT (WITH CONFIDENCE SCORE) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>FINAL SCORE</th>\n",
       "      <th>Confidence Score</th>\n",
       "      <th>WPM</th>\n",
       "      <th>Fillers</th>\n",
       "      <th>Hedging Words</th>\n",
       "      <th>Feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interview_question_1</td>\n",
       "      <td>29.1</td>\n",
       "      <td>84.2</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>✅ Penyampaian cukup baik, namun kurangi kata pengisi (filler). (Bicara sedikit lebih cepat agar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interview_question_2</td>\n",
       "      <td>25.1</td>\n",
       "      <td>77.4</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>✅ Penyampaian cukup baik, namun kurangi kata pengisi (filler). (Bicara sedikit lebih cepat agar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interview_question_3</td>\n",
       "      <td>28.4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>⭐ Terdengar sangat percaya diri dan profesional. (Bicara sedikit lebih cepat agar lebih energik)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Video ID  FINAL SCORE  Confidence Score  WPM  Fillers  \\\n",
       "0  interview_question_1         29.1              84.2   86        3   \n",
       "1  interview_question_2         25.1              77.4   74        5   \n",
       "2  interview_question_3         28.4              98.0  105        1   \n",
       "\n",
       "   Hedging Words  \\\n",
       "0              1   \n",
       "1              0   \n",
       "2              0   \n",
       "\n",
       "                                                                                              Feedback  \n",
       "0  ✅ Penyampaian cukup baik, namun kurangi kata pengisi (filler). (Bicara sedikit lebih cepat agar ...  \n",
       "1  ✅ Penyampaian cukup baik, namun kurangi kata pengisi (filler). (Bicara sedikit lebih cepat agar ...  \n",
       "2  ⭐ Terdengar sangat percaya diri dan profesional. (Bicara sedikit lebih cepat agar lebih energik)...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HASIL TRANSKRIP VIDEO ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video ID</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interview_question_1</td>\n",
       "      <td>Okay, share this specific challenges you faced while walking on the location and how you overcome them. Ah, okay, actually for this challenge just, there are some challenges when I took the starting locations, especially for the project submission that I already working with it. The first one is actually to meet the specific accuracy or population loss for the aggression metrics. And yeah, actually that's just need to take some trial and error with the different architecture. For example, we can try to add more linear, more neurons, changes to the neurons. Or even I also apply the dropout layer. So yeah, it really helps with the the validation loss to become more lower. Right. And yeah, I think that's one of the biggest challenges that I have while walking on these specifications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interview_question_2</td>\n",
       "      <td>Can you describe your experience with transfer learning and then software how to benefit projects? Ah, okay. About transfer learning is actually we use existing training model from TensorFlow for example like VGC 16, VG G90 and right. Especially for some cases that we need to use GBLerning using Keras applications. For example like image classification we can use transfer learning models which is that's already trained model with exceptionally high accuracy, high performance. Even though it's trained with different data sets but it really helps to improve our model accuracy model loss. For example like mobile net, VGG 19, VGG 16, yeah, efficient net it will help to improve our models comparing to the one if we use traditional CNN model. CNN model with the convolutional 2D, max pulling and yeah it's quite good actually to use transfer learning through the helps with our model performance to improve our model performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interview_question_3</td>\n",
       "      <td>Wait, what is this? We scroll the complexity of the form model. You have built and steps you to to ensure it's a QCN. Reference C. Um, um, um, complexity of the form model you have built. And steps you to to ensure it's accuracy. Um, okay, I will take one of my previous project that I use, yeah, I also use the RISUS project for my undergraduate thesis, yeah, for my script C. And I use this model. It's quite challenging, even though it's a very simple project. I use the RISUS project for my undergraduate thesis, yeah, for my script C. And I use this model. It's quite challenging, even though it's a chief high accuracy, yeah, with some dense layer, yeah, with some trow out layer and try to narrow it also with the callback function, yeah, with the new runs. But the problem is the data set is not balanced, yeah. So it has the in-balance class this data sets, yeah, and the the process that I use is to, yeah, to just to use the technique called smooth and, yeah, synthetic of sampling technique with editor-ners neighbor, yeah, which is basically it's just over-sampling and the data sets, yeah. It helps with accuracy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Video ID  \\\n",
       "0  interview_question_1   \n",
       "1  interview_question_2   \n",
       "2  interview_question_3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Transcript  \n",
       "0                                                                                                                                                                                                                                                                                                                                                  Okay, share this specific challenges you faced while walking on the location and how you overcome them. Ah, okay, actually for this challenge just, there are some challenges when I took the starting locations, especially for the project submission that I already working with it. The first one is actually to meet the specific accuracy or population loss for the aggression metrics. And yeah, actually that's just need to take some trial and error with the different architecture. For example, we can try to add more linear, more neurons, changes to the neurons. Or even I also apply the dropout layer. So yeah, it really helps with the the validation loss to become more lower. Right. And yeah, I think that's one of the biggest challenges that I have while walking on these specifications.  \n",
       "1                                                                                                                                                                                                    Can you describe your experience with transfer learning and then software how to benefit projects? Ah, okay. About transfer learning is actually we use existing training model from TensorFlow for example like VGC 16, VG G90 and right. Especially for some cases that we need to use GBLerning using Keras applications. For example like image classification we can use transfer learning models which is that's already trained model with exceptionally high accuracy, high performance. Even though it's trained with different data sets but it really helps to improve our model accuracy model loss. For example like mobile net, VGG 19, VGG 16, yeah, efficient net it will help to improve our models comparing to the one if we use traditional CNN model. CNN model with the convolutional 2D, max pulling and yeah it's quite good actually to use transfer learning through the helps with our model performance to improve our model performance.  \n",
       "2  Wait, what is this? We scroll the complexity of the form model. You have built and steps you to to ensure it's a QCN. Reference C. Um, um, um, complexity of the form model you have built. And steps you to to ensure it's accuracy. Um, okay, I will take one of my previous project that I use, yeah, I also use the RISUS project for my undergraduate thesis, yeah, for my script C. And I use this model. It's quite challenging, even though it's a very simple project. I use the RISUS project for my undergraduate thesis, yeah, for my script C. And I use this model. It's quite challenging, even though it's a chief high accuracy, yeah, with some dense layer, yeah, with some trow out layer and try to narrow it also with the callback function, yeah, with the new runs. But the problem is the data set is not balanced, yeah. So it has the in-balance class this data sets, yeah, and the the process that I use is to, yeah, to just to use the technique called smooth and, yeah, synthetic of sampling technique with editor-ners neighbor, yeah, which is basically it's just over-sampling and the data sets, yeah. It helps with accuracy.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    df_report = proses_penilaian_interview_lengkap()\n",
    "    \n",
    "    print(\"\\n=== AI INTERVIEW REPORT (WITH CONFIDENCE SCORE) ===\")\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    \n",
    "    # Kolom yang ditampilkan diperbarui\n",
    "    cols_to_show = ['Video ID', 'FINAL SCORE', 'Confidence Score', 'WPM', 'Fillers', 'Hedging Words', 'Feedback']\n",
    "    \n",
    "    display(df_report[cols_to_show])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Menampilkan Video ID dan Transkrip lengkapnya saja\n",
    "pd.set_option('display.max_colwidth', None) # Agar teks tidak terpotong (...)\n",
    "\n",
    "# Tampilkan tabel khusus transkrip\n",
    "print(\"=== HASIL TRANSKRIP VIDEO ===\")\n",
    "display(df_report[['Video ID', 'Transcript']])\n",
    "\n",
    "# Jika ingin menyimpan khusus transkrip ke Excel\n",
    "# df_report[['Video ID', 'Transcript']].to_excel('Transkrip_Only.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
