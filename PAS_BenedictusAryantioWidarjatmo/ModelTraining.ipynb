{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc063039",
   "metadata": {},
   "source": [
    "**LIBRARY YANG DIGUNAKAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2a02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# imblearn optional\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# gensim for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# joblib\n",
    "import joblib\n",
    "\n",
    "# nltk for augmentation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070da19",
   "metadata": {},
   "source": [
    "**LOAD DATASET & PEMBERIAN 3 LABEL TERHADAP RATING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f4f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (3235, 4)\n",
      "after mapping shape: (3235, 2)\n",
      "sentiment\n",
      "positive    3170\n",
      "negative      37\n",
      "neutral       28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tokopedia_reviews.csv')\n",
    "print(\"raw shape:\", df.shape)\n",
    "df = df.dropna(subset=['review', 'rating']).copy()\n",
    "\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "df = df.dropna(subset=['rating'])\n",
    "\n",
    "df['rating'] = df['rating'].astype(int)\n",
    "\n",
    "# Map to 3 classes\n",
    "def map_three(r):\n",
    "    if r >= 4:\n",
    "        return 'positive'\n",
    "    elif r == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(map_three)\n",
    "\n",
    "# Keep needed columns\n",
    "df = df[['review', 'sentiment']].reset_index(drop=True)\n",
    "print(\"after mapping shape:\", df.shape)\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f652e",
   "metadata": {},
   "source": [
    "**TEXT PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9ec7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_text(s):\n",
    "    s = str(s).lower()\n",
    "    # remove URLs\n",
    "    s = re.sub(r'http\\S+|www\\S+',' ', s)\n",
    "    # remove non-alphanumeric (allow spaces)\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['clean_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb43c72",
   "metadata": {},
   "source": [
    "**Augmentation & ensure >=10k samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9026d583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset < 10000, augmenting (simple) to 10000 (may be noisy). Current: 3235\n",
      "After augment: 10000\n"
     ]
    }
   ],
   "source": [
    "def synonym_replace(sentence, n_replace=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 3:\n",
    "        return sentence\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n_replace):\n",
    "        idx = random.randrange(len(words))\n",
    "        w = words[idx]\n",
    "        syns = []\n",
    "        for syn in wordnet.synsets(w):\n",
    "            for lemma in syn.lemmas():\n",
    "                cand = lemma.name().replace('_',' ')\n",
    "                if cand != w and cand.isalpha():\n",
    "                    syns.append(cand)\n",
    "        if syns:\n",
    "            new_words[idx] = random.choice(syns)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def augment_dataset(df, target_n=10000):\n",
    "    df_aug = df.copy()\n",
    "    current = len(df_aug)\n",
    "    i = 0\n",
    "    rows = df_aug.to_dict('records')\n",
    "    while current < target_n and i < len(rows):\n",
    "        row = rows[i]\n",
    "        new_text = synonym_replace(row['clean_review'], n_replace=1)\n",
    "        if new_text != row['clean_review']:\n",
    "            rows.append({'review': row.get('review',''), 'sentiment': row['sentiment'], 'clean_review': new_text})\n",
    "            current += 1\n",
    "        i += 1\n",
    "        if i >= len(rows): break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# contoh penggunaan:\n",
    "if len(df) < 10000:\n",
    "    print(\"Dataset < 10000, augmenting (simple) to 10000 (may be noisy). Current:\", len(df))\n",
    "    df = augment_dataset(df, target_n=10000)\n",
    "    print(\"After augment:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b31636",
   "metadata": {},
   "source": [
    "**Ensure class balance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b1d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before: Counter({'positive': 9462, 'negative': 493, 'neutral': 45})\n",
      "Class distribution after: Counter({'positive': 9462, 'neutral': 9462, 'negative': 9462})\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution before:\", Counter(df['sentiment']))\n",
    "# If imbalance, use RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_ros = df[['clean_review']].values\n",
    "y_ros = df['sentiment'].values\n",
    "X_res, y_res = ros.fit_resample(X_ros, y_ros)\n",
    "df_bal = pd.DataFrame({'clean_review': X_res.flatten(), 'sentiment': y_res})\n",
    "print(\"Class distribution after:\", Counter(df_bal['sentiment']))\n",
    "df = df_bal  # use balanced df going forward (optional, recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7646b",
   "metadata": {},
   "source": [
    "**Encode labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de5b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['sentiment'])  # e.g. negative->0, neutral->1, positive->2\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a5699",
   "metadata": {},
   "source": [
    "**Experiment A — LSTM with Word2Vec embeddings (80/20)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94058348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/miniconda3/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,178,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m1,178,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,178,200</span> (4.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,178,200\u001b[0m (4.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,178,200</span> (4.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,178,200\u001b[0m (4.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "texts = df['clean_review'].tolist()\n",
    "labels = df['label'].values\n",
    "\n",
    "# Split 80/20\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=RANDOM_STATE, stratify=labels)\n",
    "\n",
    "# Train Word2Vec\n",
    "sentences = [t.split() for t in X_train_texts]\n",
    "w2v_size = 200\n",
    "w2v_window = 5\n",
    "w2v_min_count = 1\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size=w2v_size, window=w2v_window, min_count=w2v_min_count, workers=4, seed=RANDOM_STATE)\n",
    "# save gensim model optionally\n",
    "os.makedirs('models', exist_ok=True)\n",
    "w2v_model.save('models/w2v.model')\n",
    "\n",
    "# Tokenizer + sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "max_len = 100  # adjust if needed\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Build embedding matrix from w2v\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(w2v_size,))  # random init for OOV\n",
    "\n",
    "# Build LSTM model\n",
    "num_classes = len(le.classes_)\n",
    "embedding_dim = w2v_size\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
    "                    input_length=max_len, trainable=False))  # set trainable True/False as experiment\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "os.makedirs('models/lstm', exist_ok=True)\n",
    "checkpoint_path = 'models/lstm/best_lstm.h5'\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=4, mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train_pad, y_train, validation_split=0.1, epochs=20, batch_size=128, callbacks=callbacks)\n",
    "\n",
    "# Evaluate\n",
    "train_loss, train_acc = model.evaluate(X_train_pad, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f\"LSTM Train acc: {train_acc:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# Save tokenizer and model\n",
    "joblib.dump(tokenizer, \"models/tokenizer.joblib\")\n",
    "model.save('models/lstm_final.h5')\n",
    "\n",
    "# Predictions & report\n",
    "y_test_pred_prob = model.predict(X_test_pad)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)\n",
    "print(\"Classification report (LSTM):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=le.classes_))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44412119",
   "metadata": {},
   "source": [
    "**Experiment B — SVM + TF-IDF (80/20)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM train acc: 0.9925136515765369\n",
      "SVM test acc : 0.9906657273687919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00      1892\n",
      "     neutral       1.00      0.97      0.99      1893\n",
      "    positive       0.97      1.00      0.99      1893\n",
      "\n",
      "    accuracy                           0.99      5678\n",
      "   macro avg       0.99      0.99      0.99      5678\n",
      "weighted avg       0.99      0.99      0.99      5678\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare TF-IDF\n",
    "X = df['clean_review'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train_svm, y_test_svm = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "svm.fit(X_train_tfidf, y_train_svm)\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred_svm = svm.predict(X_train_tfidf)\n",
    "y_test_pred_svm = svm.predict(X_test_tfidf)\n",
    "print(\"SVM train acc:\", accuracy_score(y_train_svm, y_train_pred_svm))\n",
    "print(\"SVM test acc :\", accuracy_score(y_test_svm, y_test_pred_svm))\n",
    "print(classification_report(y_test_svm, y_test_pred_svm, target_names=le.classes_))\n",
    "\n",
    "# Save model + vectorizer\n",
    "joblib.dump(svm, \"models/svm_tfidf.joblib\")\n",
    "joblib.dump(tfidf, \"models/tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe46bf",
   "metadata": {},
   "source": [
    "**Experiment C — Random Forest + TF-IDF (70/30)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train acc: 0.9990437845998994\n",
      "RF test acc : 0.9914279004227337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00      2838\n",
      "     neutral       0.97      1.00      0.99      2839\n",
      "    positive       1.00      0.97      0.99      2839\n",
      "\n",
      "    accuracy                           0.99      8516\n",
      "   macro avg       0.99      0.99      0.99      8516\n",
      "weighted avg       0.99      0.99      0.99      8516\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/tfidf_vectorizer_rf.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 70/30 split\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(1,2), max_features=35000)\n",
    "X_train_rf_tfidf = tfidf2.fit_transform(X_train_rf)\n",
    "X_test_rf_tfidf = tfidf2.transform(X_test_rf)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X_train_rf_tfidf, y_train_rf)\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred_rf = rf.predict(X_train_rf_tfidf)\n",
    "y_test_pred_rf = rf.predict(X_test_rf_tfidf)\n",
    "print(\"RF train acc:\", accuracy_score(y_train_rf, y_train_pred_rf))\n",
    "print(\"RF test acc :\", accuracy_score(y_test_rf, y_test_pred_rf))\n",
    "print(classification_report(y_test_rf, y_test_pred_rf, target_names=le.classes_))\n",
    "\n",
    "# Save\n",
    "joblib.dump(rf, \"models/rf_tfidf.joblib\")\n",
    "joblib.dump(tfidf2, \"models/tfidf_vectorizer_rf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a443a3",
   "metadata": {},
   "source": [
    "**Inference example (demonstrasi & bukti)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddfcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "TEXT: Barang sesuai deskripsi, cepat sampai dan berkualitas\n",
      "PRED: positive PROBS: [4.2448445e-08 3.5914331e-11 1.0000000e+00]\n",
      "---\n",
      "TEXT: Produk rusak dan tidak ada garansi\n",
      "PRED: positive PROBS: [1.4217164e-03 4.1488394e-05 9.9853683e-01]\n",
      "---\n",
      "TEXT: Pengiriman agak lama, produknya lumayan\n",
      "PRED: positive PROBS: [1.0224595e-04 7.9046617e-07 9.9989700e-01]\n",
      "---\n",
      "['positive' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Example inference function for LSTM (load saved tokenizer and model)\n",
    "\n",
    "\n",
    "def inference_lstm(texts, model_path='models/lstm_final.h5', tokenizer_path='models/tokenizer.joblib', max_len=100):\n",
    "    tok = joblib.load(tokenizer_path)\n",
    "    model = load_model(model_path)\n",
    "    seq = tok.texts_to_sequences([preprocess_text(t) for t in texts])\n",
    "    pad = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "    probs = model.predict(pad)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    labels = le.inverse_transform(preds)\n",
    "    return labels, probs\n",
    "\n",
    "# Example usage\n",
    "sample_texts = [\n",
    "    \"Barang sesuai deskripsi, cepat sampai dan berkualitas\",\n",
    "    \"Produk rusak dan tidak ada garansi\",\n",
    "    \"Pengiriman agak lama, produknya lumayan\"\n",
    "]\n",
    "labels, probs = inference_lstm(sample_texts)\n",
    "for t, l, p in zip(sample_texts, labels, probs):\n",
    "    print(\"TEXT:\", t)\n",
    "    print(\"PRED:\", l, \"PROBS:\", p)\n",
    "    print(\"---\")\n",
    "\n",
    "# Example inference for SVM\n",
    "def inference_svm(texts, model_path=\"models/svm_tfidf.joblib\", vec_path=\"models/tfidf_vectorizer.joblib\"):\n",
    "    clf = joblib.load(model_path)\n",
    "    vec = joblib.load(vec_path)\n",
    "    Xv = vec.transform([preprocess_text(t) for t in texts])\n",
    "    preds = clf.predict(Xv)\n",
    "    labels = le.inverse_transform(preds)\n",
    "    return labels\n",
    "\n",
    "print(inference_svm(sample_texts))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
